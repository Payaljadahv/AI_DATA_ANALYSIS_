{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automating Data Cleaning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "    Task: Basic Pipeline with Scaling\n",
    "1. Objective: Create a pipeline that scales numerical features in a dataset.\n",
    "2. Steps:\n",
    "    - Load a sample dataset with Pandas.\n",
    "    - Define a pipeline using Pipeline from sklearn.pipeline .\n",
    "    - Use StandardScaler to scale features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0          -0.900681          1.019004          -1.340227         -1.315444\n",
      "1          -1.143017         -0.131979          -1.340227         -1.315444\n",
      "2          -1.385353          0.328414          -1.397064         -1.315444\n",
      "3          -1.506521          0.098217          -1.283389         -1.315444\n",
      "4          -1.021849          1.249201          -1.340227         -1.315444\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Load sample dataset\n",
    "# Using Iris dataset for demonstration\n",
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Step 2: Define a pipeline with StandardScaler\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Step 3: Apply pipeline to scale features\n",
    "scaled_features = pipeline.fit_transform(df)\n",
    "\n",
    "# Convert back to DataFrame for inspection\n",
    "scaled_df = pd.DataFrame(scaled_features, columns=df.columns)\n",
    "\n",
    "print(scaled_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Task: Pipeline with Imputation\n",
    "1. Objective: Automate data cleaning by handling missing values.\n",
    "2. Steps:\n",
    "    - Load a dataset with missing values.\n",
    "    - Define a pipeline to use SimpleImputer for filling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before imputation:\n",
      " MedInc         2038\n",
      "HouseAge       2108\n",
      "AveRooms       2062\n",
      "AveBedrms      2033\n",
      "Population     2074\n",
      "AveOccup       2014\n",
      "Latitude       2077\n",
      "Longitude      2179\n",
      "MedHouseVal    2055\n",
      "dtype: int64\n",
      "\n",
      "Missing values after imputation:\n",
      " MedInc         0\n",
      "HouseAge       0\n",
      "AveRooms       0\n",
      "AveBedrms      0\n",
      "Population     0\n",
      "AveOccup       0\n",
      "Latitude       0\n",
      "Longitude      0\n",
      "MedHouseVal    0\n",
      "dtype: int64\n",
      "\n",
      "First 5 rows of imputed data:\n",
      "    MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup   Latitude  \\\n",
      "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556  37.880000   \n",
      "1  8.3014      21.0  6.238137   0.971880      2401.0  3.086691  35.629523   \n",
      "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260  37.850000   \n",
      "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945  37.850000   \n",
      "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467  37.850000   \n",
      "\n",
      "    Longitude  MedHouseVal  \n",
      "0 -122.230000        4.526  \n",
      "1 -119.565726        3.585  \n",
      "2 -122.240000        3.521  \n",
      "3 -119.565726        3.413  \n",
      "4 -119.565726        3.422  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load California housing dataset\n",
    "housing = fetch_california_housing(as_frame=True)\n",
    "df = housing.frame\n",
    "\n",
    "# Introduce missing values artificially for demonstration (~10%)\n",
    "np.random.seed(0)\n",
    "mask = np.random.rand(*df.shape) < 0.1\n",
    "df[mask] = np.nan\n",
    "\n",
    "print(\"Missing values before imputation:\\n\", df.isnull().sum())\n",
    "\n",
    "# Step 2: Define pipeline with SimpleImputer to fill missing values (mean)\n",
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Step 3: Fit and transform the dataset\n",
    "df_imputed = pipeline.fit_transform(df)\n",
    "\n",
    "# Convert to DataFrame with original columns\n",
    "df_imputed = pd.DataFrame(df_imputed, columns=df.columns)\n",
    "\n",
    "print(\"\\nMissing values after imputation:\\n\", df_imputed.isnull().sum())\n",
    "print(\"\\nFirst 5 rows of imputed data:\\n\", df_imputed.head())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
